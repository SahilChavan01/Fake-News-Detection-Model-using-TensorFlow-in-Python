{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Fake news involves disseminating misleading information that can lead people astray, with potentially serious real-world consequences. The goal of fake news is often to deceive, capture attention, manipulate public opinion, or harm reputations. Detecting fake news is crucial, especially for media outlets that rely on attracting viewers to their websites to generate online advertising revenue. In this project, we will develop a deep learning model using TensorFlow to detect whether news articles are fake or real.\n",
    "\n",
    "We will utilize the fake_news_dataset, which includes news texts and their corresponding labels (FAKE or REAL). The dataset can be downloaded from the provided link.\n",
    "\n",
    "The steps involved in this process are as follows:\n",
    "1. Importing Libraries and Dataset\n",
    "2. Preprocessing the Dataset\n",
    "3. Generating Word Embeddings\n",
    "4. Designing the Model Architecture\n",
    "5. Model Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Importing Libraries and Dataset\n",
    "We will use the following libraries:\n",
    "- NumPy: To handle various mathematical operations.\n",
    "- Pandas: To load and manipulate the dataset.\n",
    "- TensorFlow: For data preprocessing and model creation.\n",
    "- scikit-learn (SkLearn): For splitting the dataset into training and testing sets, and for importing modules necessary for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee52cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import pprint\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\Sahil Chavan\\Downloads\\news.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50321c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Preprocessing Dataset:\n",
    "The dataset includes an unnamed column that we need to remove. We will drop this column to clean the dataset before further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b4cf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop([\"Unnamed: 0\"], axis=1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1079ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Data Encoding:\n",
    "This step involves transforming the categorical column (label in our case) into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc44a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['label'])\n",
    "data['label'] = le.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be6506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "max_length = 54\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 3000\n",
    "test_portion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Tokenization:\n",
    "This process involves breaking down a large block of continuous text into smaller, distinct units or tokens. For improved accuracy, we handle each column separately in a sequential manner as part of our processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87d7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "text = []\n",
    "labels = []\n",
    "for x in range(training_size):\n",
    "    title.append(data['title'][x])\n",
    "    text.append(data['text'][x])\n",
    "    labels.append(data['label'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4c0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(title)\n",
    "word_index1 = tokenizer1.word_index\n",
    "vocab_size1 = len(word_index1)\n",
    "sequences1 = tokenizer1.texts_to_sequences(title)\n",
    "padded1 = pad_sequences(sequences1, padding=padding_type, truncating=trunc_type)\n",
    "split = int(test_portion * training_size)\n",
    "training_sequences1 = padded1[split:training_size]\n",
    "test_sequences1 = padded1[0:split]\n",
    "test_labels = labels[0:split]\n",
    "training_labels = labels[split:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Generating Word Embeddings:\n",
    "Word embeddings map words with similar meanings to similar representations. In this approach, each word is represented as a real-valued vector within a predefined vector space. We will use the `glove.6B.50d.txt` file, which provides this predefined vector space for words. You can download the file using this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95342a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "file_path = r\"C:\\Users\\Sahil Chavan\\Downloads\\glove.6B.50d.txt\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Unicode decode error: {e}\")\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size1 + 1, embedding_dim))\n",
    "\n",
    "word_index1 = {'example': 1}\n",
    "\n",
    "for word, i in word_index1.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Creating Model Architecture:\n",
    "We will now use TensorFlow to build our model. Specifically, we'll employ the TensorFlow embedding technique via the Keras Embedding Layer, which transforms the original input data into a set of real-valued vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d767b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 54, 50)            500050    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 54, 50)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 50, 64)            16064     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 12, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 549203 (2.10 MB)\n",
      "Trainable params: 49153 (192.00 KB)\n",
      "Non-trainable params: 500050 (1.91 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size1+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix],trainable=False), \n",
    "                             tf.keras.layers.Dropout(0.2),\n",
    "                             tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "                             tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "                             tf.keras.layers.LSTM(64),\n",
    "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "                             ])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24bd6c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "2700/2700 - 2s - loss: 0.6923 - accuracy: 0.5100 - val_loss: 0.6884 - val_accuracy: 0.5667 - 2s/epoch - 822us/sample\n",
      "Epoch 2/50\n",
      "2700/2700 - 1s - loss: 0.6908 - accuracy: 0.5237 - val_loss: 0.6874 - val_accuracy: 0.5633 - 755ms/epoch - 280us/sample\n",
      "Epoch 3/50\n",
      "2700/2700 - 1s - loss: 0.6913 - accuracy: 0.5278 - val_loss: 0.6885 - val_accuracy: 0.5633 - 782ms/epoch - 290us/sample\n",
      "Epoch 4/50\n",
      "2700/2700 - 1s - loss: 0.6909 - accuracy: 0.5215 - val_loss: 0.6849 - val_accuracy: 0.5667 - 811ms/epoch - 301us/sample\n",
      "Epoch 5/50\n",
      "2700/2700 - 1s - loss: 0.6912 - accuracy: 0.5222 - val_loss: 0.6861 - val_accuracy: 0.5667 - 855ms/epoch - 317us/sample\n",
      "Epoch 6/50\n",
      "2700/2700 - 1s - loss: 0.6911 - accuracy: 0.5219 - val_loss: 0.6863 - val_accuracy: 0.5633 - 943ms/epoch - 349us/sample\n",
      "Epoch 7/50\n",
      "2700/2700 - 1s - loss: 0.6910 - accuracy: 0.5204 - val_loss: 0.6854 - val_accuracy: 0.5667 - 973ms/epoch - 360us/sample\n",
      "Epoch 8/50\n",
      "2700/2700 - 1s - loss: 0.6907 - accuracy: 0.5248 - val_loss: 0.6844 - val_accuracy: 0.5667 - 939ms/epoch - 348us/sample\n",
      "Epoch 9/50\n",
      "2700/2700 - 1s - loss: 0.6902 - accuracy: 0.5241 - val_loss: 0.6852 - val_accuracy: 0.5667 - 1s/epoch - 375us/sample\n",
      "Epoch 10/50\n",
      "2700/2700 - 1s - loss: 0.6898 - accuracy: 0.5252 - val_loss: 0.6841 - val_accuracy: 0.5667 - 1s/epoch - 416us/sample\n",
      "Epoch 11/50\n",
      "2700/2700 - 1s - loss: 0.6903 - accuracy: 0.5263 - val_loss: 0.6863 - val_accuracy: 0.5700 - 1s/epoch - 519us/sample\n",
      "Epoch 12/50\n",
      "2700/2700 - 1s - loss: 0.6902 - accuracy: 0.5185 - val_loss: 0.6850 - val_accuracy: 0.5700 - 1s/epoch - 500us/sample\n",
      "Epoch 13/50\n",
      "2700/2700 - 1s - loss: 0.6902 - accuracy: 0.5263 - val_loss: 0.6834 - val_accuracy: 0.5667 - 1s/epoch - 526us/sample\n",
      "Epoch 14/50\n",
      "2700/2700 - 1s - loss: 0.6902 - accuracy: 0.5252 - val_loss: 0.6856 - val_accuracy: 0.5700 - 1s/epoch - 513us/sample\n",
      "Epoch 15/50\n",
      "2700/2700 - 1s - loss: 0.6903 - accuracy: 0.5289 - val_loss: 0.6873 - val_accuracy: 0.5700 - 1s/epoch - 480us/sample\n",
      "Epoch 16/50\n",
      "2700/2700 - 1s - loss: 0.6900 - accuracy: 0.5285 - val_loss: 0.6856 - val_accuracy: 0.5700 - 1s/epoch - 464us/sample\n",
      "Epoch 17/50\n",
      "2700/2700 - 1s - loss: 0.6892 - accuracy: 0.5296 - val_loss: 0.6849 - val_accuracy: 0.5733 - 1s/epoch - 448us/sample\n",
      "Epoch 18/50\n",
      "2700/2700 - 1s - loss: 0.6891 - accuracy: 0.5270 - val_loss: 0.6832 - val_accuracy: 0.5733 - 1s/epoch - 436us/sample\n",
      "Epoch 19/50\n",
      "2700/2700 - 1s - loss: 0.6892 - accuracy: 0.5289 - val_loss: 0.6838 - val_accuracy: 0.5700 - 1s/epoch - 413us/sample\n",
      "Epoch 20/50\n",
      "2700/2700 - 1s - loss: 0.6886 - accuracy: 0.5278 - val_loss: 0.6828 - val_accuracy: 0.5700 - 1s/epoch - 373us/sample\n",
      "Epoch 21/50\n",
      "2700/2700 - 1s - loss: 0.6884 - accuracy: 0.5307 - val_loss: 0.6831 - val_accuracy: 0.5700 - 1s/epoch - 381us/sample\n",
      "Epoch 22/50\n",
      "2700/2700 - 1s - loss: 0.6880 - accuracy: 0.5330 - val_loss: 0.6842 - val_accuracy: 0.5700 - 1s/epoch - 374us/sample\n",
      "Epoch 23/50\n",
      "2700/2700 - 1s - loss: 0.6873 - accuracy: 0.5289 - val_loss: 0.6844 - val_accuracy: 0.5667 - 954ms/epoch - 353us/sample\n",
      "Epoch 24/50\n",
      "2700/2700 - 1s - loss: 0.6873 - accuracy: 0.5319 - val_loss: 0.6851 - val_accuracy: 0.5633 - 1s/epoch - 377us/sample\n",
      "Epoch 25/50\n",
      "2700/2700 - 1s - loss: 0.6861 - accuracy: 0.5326 - val_loss: 0.6855 - val_accuracy: 0.5633 - 1s/epoch - 390us/sample\n",
      "Epoch 26/50\n",
      "2700/2700 - 1s - loss: 0.6858 - accuracy: 0.5196 - val_loss: 0.6828 - val_accuracy: 0.5667 - 1s/epoch - 396us/sample\n",
      "Epoch 27/50\n",
      "2700/2700 - 1s - loss: 0.6857 - accuracy: 0.5381 - val_loss: 0.6834 - val_accuracy: 0.5700 - 1s/epoch - 380us/sample\n",
      "Epoch 28/50\n",
      "2700/2700 - 1s - loss: 0.6859 - accuracy: 0.5374 - val_loss: 0.6886 - val_accuracy: 0.5600 - 1s/epoch - 374us/sample\n",
      "Epoch 29/50\n",
      "2700/2700 - 1s - loss: 0.6862 - accuracy: 0.5344 - val_loss: 0.6850 - val_accuracy: 0.5700 - 995ms/epoch - 368us/sample\n",
      "Epoch 30/50\n",
      "2700/2700 - 1s - loss: 0.6855 - accuracy: 0.5126 - val_loss: 0.6830 - val_accuracy: 0.5700 - 998ms/epoch - 370us/sample\n",
      "Epoch 31/50\n",
      "2700/2700 - 1s - loss: 0.6840 - accuracy: 0.5370 - val_loss: 0.6864 - val_accuracy: 0.5600 - 1s/epoch - 394us/sample\n",
      "Epoch 32/50\n",
      "2700/2700 - 1s - loss: 0.6840 - accuracy: 0.5330 - val_loss: 0.6832 - val_accuracy: 0.5633 - 1s/epoch - 428us/sample\n",
      "Epoch 33/50\n",
      "2700/2700 - 1s - loss: 0.6833 - accuracy: 0.5374 - val_loss: 0.6823 - val_accuracy: 0.5733 - 1s/epoch - 409us/sample\n",
      "Epoch 34/50\n",
      "2700/2700 - 1s - loss: 0.6830 - accuracy: 0.5337 - val_loss: 0.6816 - val_accuracy: 0.5733 - 1s/epoch - 419us/sample\n",
      "Epoch 35/50\n",
      "2700/2700 - 1s - loss: 0.6835 - accuracy: 0.5381 - val_loss: 0.6825 - val_accuracy: 0.5700 - 1s/epoch - 436us/sample\n",
      "Epoch 36/50\n",
      "2700/2700 - 1s - loss: 0.6823 - accuracy: 0.5381 - val_loss: 0.6910 - val_accuracy: 0.5600 - 1s/epoch - 463us/sample\n",
      "Epoch 37/50\n",
      "2700/2700 - 1s - loss: 0.6824 - accuracy: 0.5252 - val_loss: 0.6890 - val_accuracy: 0.5600 - 1s/epoch - 449us/sample\n",
      "Epoch 38/50\n",
      "2700/2700 - 1s - loss: 0.6818 - accuracy: 0.5356 - val_loss: 0.6825 - val_accuracy: 0.5700 - 1s/epoch - 435us/sample\n",
      "Epoch 39/50\n",
      "2700/2700 - 1s - loss: 0.6819 - accuracy: 0.5356 - val_loss: 0.6885 - val_accuracy: 0.5600 - 1s/epoch - 409us/sample\n",
      "Epoch 40/50\n",
      "2700/2700 - 1s - loss: 0.6817 - accuracy: 0.5344 - val_loss: 0.6830 - val_accuracy: 0.5700 - 1s/epoch - 412us/sample\n",
      "Epoch 41/50\n",
      "2700/2700 - 1s - loss: 0.6808 - accuracy: 0.5326 - val_loss: 0.6885 - val_accuracy: 0.5567 - 1s/epoch - 431us/sample\n",
      "Epoch 42/50\n",
      "2700/2700 - 1s - loss: 0.6812 - accuracy: 0.5411 - val_loss: 0.6878 - val_accuracy: 0.5633 - 1s/epoch - 429us/sample\n",
      "Epoch 43/50\n",
      "2700/2700 - 1s - loss: 0.6806 - accuracy: 0.5356 - val_loss: 0.6925 - val_accuracy: 0.5600 - 1s/epoch - 459us/sample\n",
      "Epoch 44/50\n",
      "2700/2700 - 1s - loss: 0.6812 - accuracy: 0.5367 - val_loss: 0.6880 - val_accuracy: 0.5667 - 1s/epoch - 487us/sample\n",
      "Epoch 45/50\n",
      "2700/2700 - 1s - loss: 0.6806 - accuracy: 0.5352 - val_loss: 0.6831 - val_accuracy: 0.5667 - 1s/epoch - 462us/sample\n",
      "Epoch 46/50\n",
      "2700/2700 - 1s - loss: 0.6799 - accuracy: 0.5363 - val_loss: 0.6856 - val_accuracy: 0.5633 - 1s/epoch - 466us/sample\n",
      "Epoch 47/50\n",
      "2700/2700 - 1s - loss: 0.6803 - accuracy: 0.5411 - val_loss: 0.6887 - val_accuracy: 0.5600 - 1s/epoch - 550us/sample\n",
      "Epoch 48/50\n",
      "2700/2700 - 1s - loss: 0.6800 - accuracy: 0.5370 - val_loss: 0.6828 - val_accuracy: 0.5700 - 1s/epoch - 453us/sample\n",
      "Epoch 49/50\n",
      "2700/2700 - 1s - loss: 0.6793 - accuracy: 0.5363 - val_loss: 0.6849 - val_accuracy: 0.5667 - 1s/epoch - 462us/sample\n",
      "Epoch 50/50\n",
      "2700/2700 - 1s - loss: 0.6803 - accuracy: 0.5352 - val_loss: 0.6858 - val_accuracy: 0.5667 - 1s/epoch - 485us/sample\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "training_padded = np.array(training_sequences1)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(test_sequences1)\n",
    "testing_labels = np.array(test_labels)\n",
    "\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Model Evaluation and Prediction:\n",
    "With the detection model successfully constructed using TensorFlow, we will now proceed to evaluate its performance. We will test the model by predicting the authenticity of some news text, determining whether it is genuine or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44b8ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This news is True\n"
     ]
    }
   ],
   "source": [
    "X = \"Karry to go France in gesture of sympathy\"\n",
    "\n",
    "sequences = tokenizer1.texts_to_sequences([X])[0]\n",
    "sequences = pad_sequences([sequences], maxlen=54, padding=padding_type, truncating=trunc_type)\n",
    "if(model.predict(sequences, verbose=0)[0][0] >= 0.5):\n",
    "    print(\"This news is True\")\n",
    "else:\n",
    "    print(\"This news is False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Conclusion:\n",
    "By following these steps, we can effectively create a fake news detection model using TensorFlow and Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
